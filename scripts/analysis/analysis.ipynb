{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for *Generating Event Descriptions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import arviz\n",
    "\n",
    "from typing import Literal, Callable\n",
    "from dataclasses import dataclass\n",
    "from glob import glob\n",
    "from scipy.stats import spearmanr\n",
    "from patsy import dmatrix\n",
    "from cmdstanpy import CmdStanModel, CmdStanMCMC, from_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Naturalness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin analysis of the naturalness experiment by loading and combining the naturalness datasets into a single `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_fnames = [\n",
    "    \"naturalness_llama-propbanksenses.csv\",\n",
    "    \"naturalness_llama-llamasenses.csv\",\n",
    "    \"naturalness_reddit.csv\",\n",
    "    \"naturalness_manual.csv\"\n",
    "]\n",
    "\n",
    "data_nat = []\n",
    "\n",
    "for fname in nat_fnames:\n",
    "    df = pd.read_csv(f\"data/{fname}\")\n",
    "\n",
    "    generation_method = fname.replace(\"naturalness_\", \"\").strip(\".csv\")\n",
    "\n",
    "    if generation_method == \"manual\":\n",
    "        df[\"list_type\"] = \"manual\"\n",
    "        df[\"generation_method_type\"] = \"manual\"\n",
    "        df[\"generation_method_subtype\"] = \"manual\"\n",
    "        df[\"generation_method_subsubtype\"] = \"manual\"\n",
    "        df = df.query(\"sentence_type == 'target'\")\n",
    "        df[\"generation_method\"] = \"manual_\" + df.naturalness + \"_\" + df.typicality\n",
    "    else:\n",
    "        df[\"list_type\"] = generation_method\n",
    "        df[\"generation_method_type\"] = df.sentence_type.map(lambda x: \"manual\" if x == \"calibration\" else \"automated\")\n",
    "        df[\"generation_method_subtype\"] = df.sentence_type.map(\n",
    "            lambda x: \"manual\" if x == \"calibration\" else generation_method.split(\"-\")[0]\n",
    "        )\n",
    "        df[\"generation_method_subsubtype\"] = df.sentence_type.map(\n",
    "            lambda x: \"manual\" if x == \"calibration\" else generation_method\n",
    "        )\n",
    "        df[\"generation_method\"] = df[[\"sentence_type\", \"naturalness\", \"typicality\"]].agg(\n",
    "            lambda x: \"manual_\" + x.naturalness + \"_\" + x.typicality if x.sentence_type == \"calibration\" else generation_method,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    data_nat.append(df)\n",
    "\n",
    "data_nat = pd.concat(data_nat)\n",
    "\n",
    "data_nat[\"surprisal_z\"] = (data_nat.surprisal - data_nat.surprisal.mean())/data_nat.surprisal.std()\n",
    "data_nat[\"freq_z\"] = (data_nat.freq - data_nat.freq.mean())/data_nat.freq.std()\n",
    "\n",
    "data_nat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant filtration\n",
    "\n",
    "Some participants did not complete the survey. We remove these participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_participants(data, expected_counts: dict[str, int]) -> pd.DataFrame:\n",
    "    \"\"\"Filter participants who did not complete the task\"\"\"\n",
    "    actual_counts = data.groupby(\"list_type\").rater_id.value_counts().reset_index()\n",
    "\n",
    "    exclude_participants = [\n",
    "        r.rater_id for _, r in actual_counts.iterrows()\n",
    "        if r[\"count\"] != expected_counts[r.list_type]\n",
    "    ]\n",
    "\n",
    "    return data[~data.rater_id.isin(exclude_participants)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_counts_nat = {\n",
    "    \"llama-llamasense\": 97,\n",
    "    \"llama-propbanksense\": 97,\n",
    "    \"manual\": 124,\n",
    "    \"reddit\": 72\n",
    "}\n",
    "\n",
    "data_nat = filter_participants(data_nat, expected_counts_nat)\n",
    "\n",
    "data_nat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the data, we use a generlized linear mixed effects model with an ordered beta link. This model is implemented in `scripts/analysis/models/ordered-beta.stan`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CmdStanModel(stan_file=\"scripts/analysis/models/ordered-beta.stan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit this model to our data, we need to map it into the format assumd by the model's `data` block.\n",
    "\n",
    "```stan\n",
    "data {\n",
    "  int<lower=1> N_resp;                             // number of responses\n",
    "  int<lower=1> N_subj;                             // number of subjects\n",
    "  int<lower=1> N_verb;                             // number of verbs\n",
    "  int<lower=1> N_sense;                            // number of senses\n",
    "  int<lower=1> N_item;                             // number of items\n",
    "  int<lower=1> N_fixed;                            // number of fixed predictors\n",
    "  int<lower=1> N_by_subj;                          // number of random by-subject predictors\n",
    "  int<lower=1> N_by_verb;                          // number of random by-verb predictors\n",
    "  int<lower=1> N_by_sense;                         // number of random by-sense predictors\n",
    "  int<lower=1> N_by_item;                          // number of random by-item predictors\n",
    "  matrix[N_resp,N_fixed] fixed_predictors;         // predictors including intercept\n",
    "  matrix[N_resp,N_by_subj] by_subj_predictors;     // by-subject predictors including intercept\n",
    "  matrix[N_resp,N_by_verb] by_verb_predictors;     // by-verb predictors including intercept\n",
    "  matrix[N_resp,N_by_sense] by_sense_predictors;   // by-sense predictors including intercept\n",
    "  matrix[N_resp,N_by_item] by_item_predictors;     // by-item predictors including intercept\n",
    "  array[N_resp] int<lower=1,upper=N_subj> subj;    // subject who gave response n\n",
    "  array[N_resp] int<lower=1,upper=N_verb> verb;    // verb corresponding to response n\n",
    "  array[N_resp] int<lower=1,upper=N_sense> sense;  // sense corresponding to response n\n",
    "  array[N_resp] int<lower=1,upper=N_item> item;    // item corresponding to response n\n",
    "  array[N_resp] int<lower=1,upper=3> resp_bin;     // whether a response is 0=1, (0, 1)=1, or 1=2\n",
    "  array[N_resp] real<lower=0,upper=1> resp;        // [0, 1] responses                                    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_response(x: float) -> int:\n",
    "    \"\"\"Bin the response by whether it is an endpoint (0, 1) or not.\"\"\"\n",
    "    if x == 0.0:\n",
    "        return 1\n",
    "    elif x == 1.0:\n",
    "        return 3\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def prepare_data(\n",
    "    data: pd.DataFrame, \n",
    "    fixed_formula: str,\n",
    "    by_subj_formula: str,\n",
    "    by_verb_formula: str,\n",
    "    by_sense_formula: str,\n",
    "    by_item_formula: str,\n",
    "    item_cols: list[str],\n",
    "    sense_cols: list[str],\n",
    "    subj_cols: list[str] = [\"rater_id\"], \n",
    "    verb_cols: list[str] = [\"verb\"], \n",
    "    resp_col: str = \"rating\",\n",
    ") -> tuple[dict[str, int | np.ndarray], np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # scale the responses to [0, 1]\n",
    "    resp = data[resp_col] / 100\n",
    "\n",
    "    # bin the responses\n",
    "    resp_bin = resp.map(bin_response)\n",
    "\n",
    "    # construct the design matrices\n",
    "    fixed_predictors = dmatrix(fixed_formula, data, return_type=\"dataframe\")\n",
    "    by_subj_predictors = dmatrix(by_subj_formula, data, return_type=\"dataframe\")\n",
    "    by_verb_predictors = dmatrix(by_verb_formula, data, return_type=\"dataframe\")\n",
    "    by_sense_predictors = dmatrix(by_sense_formula, data, return_type=\"dataframe\")\n",
    "    by_item_predictors = dmatrix(by_item_formula, data, return_type=\"dataframe\")\n",
    "\n",
    "    # hash the items and subjects\n",
    "    subjid = data[subj_cols].agg('-'.join, axis=1).astype(\"category\")\n",
    "    verbid = data[verb_cols].agg('-'.join, axis=1).astype(\"category\")\n",
    "    senseid = data[sense_cols].agg('-'.join, axis=1).astype(\"category\")\n",
    "    itemid = data[item_cols].agg('-'.join, axis=1).astype(\"category\")\n",
    "    \n",
    "    # determine whether the subject is a discrete responder\n",
    "    disc_responder = data[[resp_col]].isin([0, 100])\n",
    "    disc_responder[\"subjid\"] = subjid\n",
    "    disc_responder = disc_responder.groupby(\"subjid\")[resp_col].all()\n",
    "\n",
    "    data_stan = {\n",
    "        \"N_resp\": data.shape[0],\n",
    "        \"N_subj\": subjid.cat.codes.max() + 1,\n",
    "        \"N_verb\": verbid.cat.codes.max() + 1,\n",
    "        \"N_sense\": senseid.cat.codes.max() + 1,\n",
    "        \"N_item\": itemid.cat.codes.max() + 1,\n",
    "        \"N_fixed\": fixed_predictors.shape[1],\n",
    "        \"N_by_subj\": by_subj_predictors.shape[1],\n",
    "        \"N_by_verb\": by_verb_predictors.shape[1],\n",
    "        \"N_by_sense\": by_sense_predictors.shape[1],\n",
    "        \"N_by_item\": by_item_predictors.shape[1],\n",
    "        \"fixed_predictors\": fixed_predictors.values,\n",
    "        \"by_subj_predictors\": by_subj_predictors.values,\n",
    "        \"by_verb_predictors\": by_verb_predictors.values,\n",
    "        \"by_sense_predictors\": by_sense_predictors.values,\n",
    "        \"by_item_predictors\": by_item_predictors.values,\n",
    "        \"subj\": subjid.cat.codes.values + 1,\n",
    "        \"verb\": verbid.cat.codes.values + 1,\n",
    "        \"sense\": senseid.cat.codes.values + 1,\n",
    "        \"item\": itemid.cat.codes.values + 1,\n",
    "        \"resp_bin\": resp_bin.values.astype(int),\n",
    "        \"resp\": resp.values\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        data_stan, \n",
    "        fixed_predictors.columns.values, \n",
    "        by_subj_predictors.columns.values,\n",
    "        by_verb_predictors.columns.values,\n",
    "        by_sense_predictors.columns.values,\n",
    "        by_item_predictors.columns.values,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimates for the distribution over fixed and random effects reported in the paper are extracted from the model fits using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_coef_stats(fit: CmdStanMCMC, coef_names: list[str]) -> pd.DataFrame:\n",
    "    fixed_coefs = fit.stan_variable(\"fixed_coefs\")\n",
    "\n",
    "    posterior_means = fixed_coefs.mean(axis=0)\n",
    "    \n",
    "    stats = pd.DataFrame(\n",
    "        np.quantile(fixed_coefs, [0.025, 0.05, 0.95,  0.975], axis=0).T, \n",
    "        index=coef_names,\n",
    "        columns=[\"2.5%\", \"5%\", \"95%\", \"97.5%\"]\n",
    "    )\n",
    "\n",
    "    stats[\"post_mean\"] = posterior_means\n",
    "    \n",
    "    stats[\"p\"] = np.mean(\n",
    "        np.sign(posterior_means)[None,:] != np.sign(fixed_coefs), \n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "    return stats[[\"post_mean\", \"2.5%\", \"5%\", \"95%\", \"97.5%\", \"p\"]]\n",
    "\n",
    "def random_coef_stats(\n",
    "    fit: CmdStanMCMC, \n",
    "    var_name: Literal[\n",
    "        \"subj_cov\", \"subj_corr\", \n",
    "        \"verb_cov\", \"verb_corr\", \n",
    "        \"sense_cov\", \"sense_corr\", \n",
    "        \"item_cov\", \"item_corr\"\n",
    "    ], \n",
    "    coef_names: list[str]\n",
    ") -> pd.DataFrame:\n",
    "    matrices = fit.stan_variable(var_name)\n",
    "    posterior_means = pd.DataFrame(\n",
    "        matrices.mean(axis=0),\n",
    "        index=coef_names,\n",
    "        columns=coef_names\n",
    "    )\n",
    "\n",
    "    return posterior_means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the actual model fits are done using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelResults:\n",
    "    fit: CmdStanMCMC\n",
    "    fixed_coefs: pd.DataFrame\n",
    "    subj_cov:  pd.DataFrame\n",
    "    verb_cov:  pd.DataFrame\n",
    "    sense_cov: pd.DataFrame\n",
    "    item_cov:  pd.DataFrame\n",
    "    subj_corr: pd.DataFrame\n",
    "    verb_corr: pd.DataFrame\n",
    "    sense_corr: pd.DataFrame\n",
    "    item_corr: pd.DataFrame\n",
    "\n",
    "def fit_hmc(\n",
    "    data: pd.DataFrame,\n",
    "    fixed_formula: str, \n",
    "    by_subj_formula: str,\n",
    "    by_verb_formula: str,\n",
    "    by_sense_formula: str,\n",
    "    by_item_formula: str,  \n",
    "    item_cols: list[str],\n",
    "    subj_cols: list[str],\n",
    "    verb_cols: list[str],\n",
    "    sense_cols: list[str], \n",
    "    seed: int = 30298,\n",
    "    **kwargs\n",
    ") -> ModelResults:\n",
    "    data_stan, fixed_predictors, by_subj_predictors, by_verb_predictors, by_sense_predictors, by_item_predictors = prepare_data(\n",
    "        data,\n",
    "        fixed_formula=fixed_formula,\n",
    "        by_subj_formula=by_subj_formula,\n",
    "        by_verb_formula=by_verb_formula,\n",
    "        by_sense_formula=by_sense_formula,\n",
    "        by_item_formula=by_item_formula, \n",
    "        subj_cols=subj_cols,\n",
    "        verb_cols=verb_cols,\n",
    "        sense_cols=sense_cols,\n",
    "        item_cols=item_cols\n",
    "    )\n",
    "\n",
    "    fit = model.sample(\n",
    "        data=data_stan,\n",
    "        seed=seed,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return ModelResults(\n",
    "        fit = fit, \n",
    "        fixed_coefs = fixed_coef_stats(fit, fixed_predictors),\n",
    "        subj_cov = random_coef_stats(fit, \"subj_cov\", by_subj_predictors),\n",
    "        verb_cov = random_coef_stats(fit, \"verb_cov\", by_verb_predictors),\n",
    "        sense_cov = random_coef_stats(fit, \"sense_cov\", by_sense_predictors),\n",
    "        item_cov = random_coef_stats(fit, \"item_cov\", by_item_predictors),\n",
    "        subj_corr = random_coef_stats(fit, \"subj_corr\", by_subj_predictors),\n",
    "        verb_corr = random_coef_stats(fit, \"verb_corr\", by_verb_predictors),\n",
    "        sense_corr = random_coef_stats(fit, \"sense_corr\", by_sense_predictors),\n",
    "        item_corr = random_coef_stats(fit, \"item_corr\", by_item_predictors)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we don't want ot have to rerun the models if we already have them cached, we'll also define a method for loading a fit from CSVs dumped by `cmdstanpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fit(\n",
    "    path: str, \n",
    "    data: pd.DataFrame,\n",
    "    fixed_formula: str, \n",
    "    by_subj_formula: str,\n",
    "    by_verb_formula: str,\n",
    "    by_sense_formula: str,\n",
    "    by_item_formula: str,  \n",
    "    item_cols: list[str],\n",
    "    subj_cols: list[str],\n",
    "    verb_cols: list[str],\n",
    "    sense_cols: list[str],\n",
    ") -> ModelResults:\n",
    "    _, fixed_predictors, by_subj_predictors, by_verb_predictors, by_sense_predictors, by_item_predictors = prepare_data(\n",
    "        data,\n",
    "        fixed_formula=fixed_formula,\n",
    "        by_subj_formula=by_subj_formula,\n",
    "        by_verb_formula=by_verb_formula,\n",
    "        by_sense_formula=by_sense_formula,\n",
    "        by_item_formula=by_item_formula, \n",
    "        subj_cols=subj_cols,\n",
    "        verb_cols=verb_cols,\n",
    "        sense_cols=sense_cols,\n",
    "        item_cols=item_cols\n",
    "    )\n",
    "\n",
    "    fit = from_csv(path, method=\"sample\")\n",
    "\n",
    "    return ModelResults(\n",
    "        fit = fit, \n",
    "        fixed_coefs = fixed_coef_stats(fit, fixed_predictors),\n",
    "        subj_cov = random_coef_stats(fit, \"subj_cov\", by_subj_predictors),\n",
    "        verb_cov = random_coef_stats(fit, \"verb_cov\", by_verb_predictors),\n",
    "        sense_cov = random_coef_stats(fit, \"sense_cov\", by_sense_predictors),\n",
    "        item_cov = random_coef_stats(fit, \"item_cov\", by_item_predictors),\n",
    "        subj_corr = random_coef_stats(fit, \"subj_corr\", by_subj_predictors),\n",
    "        verb_corr = random_coef_stats(fit, \"verb_corr\", by_verb_predictors),\n",
    "        sense_corr = random_coef_stats(fit, \"sense_corr\", by_sense_predictors),\n",
    "        item_corr = random_coef_stats(fit, \"item_corr\", by_item_predictors)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following sampler parameters throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_params = {\n",
    "    \"iter_warmup\": 2500, \n",
    "    \"iter_sampling\": 2500\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to enforce a particular coding of the generation levels, we specify that ordering as a list that will be passed to `patsy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_levels = [\n",
    "    'manual_natural_typical', 'manual_natural_atypical', \n",
    "    'manual_unnatural_typical', 'manual_unnatural_atypical',\n",
    "    'reddit', 'llama-propbanksense', 'llama-llamasense'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we actually fit the model (or load it if we have a cached fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if glob(\"fits/nat/base/*.csv\"):\n",
    "    results_nat = load_fit(\n",
    "        \"fits/nat/base/*.csv\", \n",
    "        data_nat,\n",
    "        fixed_formula=\"~ 1 + C(generation_method, levels=generation_levels)\",\n",
    "        by_subj_formula=\"~ 1\", # cannot fit anything bigger, because not all subjects saw items from every generation method\n",
    "        by_verb_formula=\"~ 1\", # cannot fit anything bigger, because not all verbs show up with each generation method\n",
    "        by_sense_formula=\"~ 1\", # cannot fit anything bigger, because not all verb senses show up with each generation method\n",
    "        by_item_formula=\"~ 1\", # cannot fit anything bigger, because items are specific to generation method\n",
    "        item_cols = [\"sentence\"],\n",
    "        subj_cols = [\"rater_id\"],\n",
    "        verb_cols = [\"verb\"],\n",
    "        sense_cols = [\"verb\", \"sense\"],\n",
    "    )\n",
    "else:\n",
    "    results_nat = fit_hmc(\n",
    "        data_nat,\n",
    "        fixed_formula=\"~ 1 + C(generation_method, levels=generation_levels)\",\n",
    "        by_subj_formula=\"~ 1\", # cannot fit anything bigger, because not all subjects saw items from every generation method\n",
    "        by_verb_formula=\"~ 1\", # cannot fit anything bigger, because not all verbs show up with each generation method\n",
    "        by_sense_formula=\"~ 1\", # cannot fit anything bigger, because not all verb senses show up with each generation method\n",
    "        by_item_formula=\"~ 1\", # cannot fit anything bigger, because items are specific to generation method\n",
    "        item_cols = [\"sentence\"],\n",
    "        subj_cols = [\"rater_id\"],\n",
    "        verb_cols = [\"verb\"],\n",
    "        sense_cols = [\"verb\", \"sense\"],\n",
    "        output_dir=\"fits/nat/base\",\n",
    "        **sampler_params\n",
    "    )\n",
    "\n",
    "    results_nat.fit.diagnose()\n",
    "\n",
    "    # handles a bug in cmdstanpy.from_csv that I suspect has to do with a version change in STAN\n",
    "    # basically, I *think* STAN used to dump the save_warmup flag as an int, but now it uses a boolean\n",
    "    !find . -type f -wholename './fits/nat/base/*.csv' | xargs sed -i 's/save_warmup = false/save_warmup = 0/g'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the fixed effects coefficient estimates by looking at the `fixed_coefs` attribute of the `ModelResults`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nat.fixed_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the $\\LaTeX$ table used in the paper using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fixed_coef_tabular(fit: ModelResults, coef_map: Callable[str, str]) -> None:\n",
    "    print(r\"\\begin{tabular}{rcccr}\")\n",
    "\n",
    "    print(r\"\\toprule\")\n",
    "    print(r\"      & \\textbf{Post. mean} & \\textbf{2.5\\%} & \\textbf{97.5\\%} & \\textbf{Post.} $p$ \\\\\")\n",
    "    print(r\"\\midrule\")\n",
    "\n",
    "    for coef, row in fit.fixed_coefs.iterrows():\n",
    "        if row.p < 0.001:\n",
    "            print(f\"{coef_map(coef)} & {row.post_mean:2.2f} & {row['2.5%']:2.2f} & {row['97.5%']:2.2f} & $<$ 0.01 \\\\\\\\\")\n",
    "        else:    \n",
    "            print(f\"{coef_map(coef)} & {row.post_mean:2.2f} & {row['2.5%']:2.2f} & {row['97.5%']:2.2f} & {row.p:2.2f} \\\\\\\\\")\n",
    "\n",
    "    print(r\"\\bottomrule\")\n",
    "    print(r\"\\end{tabular}\")\n",
    "\n",
    "coef_map = {\n",
    "    \"manual_unnatural_typical\": \"Manual (Unnatural \\\\& Typical)\",\n",
    "    \"manual_natural_typical\": \"Manual (Natural \\\\& Typical )\",\n",
    "    \"manual_unnatural_atypical\": \"Manual (Unnatural \\\\& Atypical)\",\n",
    "    \"manual_natural_atypical\": \"Manual (Natural \\\\& Atypical)\",\n",
    "    \"reddit\": \"Corpus\",\n",
    "    \"llama-propbanksense\": \"LM with PropBank senses\",\n",
    "    \"llama-llamasense\": \"LM with LM senses\",\n",
    "    \"llama\": \"LM\"\n",
    "}\n",
    "\n",
    "def process_coef_name_nat_typ(coef: str) -> str:\n",
    "    if coef == \"Intercept\":\n",
    "        return coef\n",
    "    else:\n",
    "        return \" $\\\\times$ \".join(\n",
    "            coef_map[v] for v in re.findall(\n",
    "                \"C\\(generation_method, levels=generation_levels\\)\\[T\\.(.*)\\]\", \n",
    "                coef\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_fixed_coef_tabular(results_nat, process_coef_name_nat_typ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutpoint distributions\n",
    "\n",
    "In the caption of Table 8 of the paper, we report estimates of the posterior distributions over cutpoints. These estimates are calculated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutpoint_stats(fit: CmdStanMCMC) -> tuple[np.ndarray, np.ndarray]: \n",
    "    cutpoint0 = fit.stan_variable(\"cutpoint0\")\n",
    "    cutpoint1 = cutpoint0 + np.exp(fit.stan_variable(\"interval_size_logmean\"))\n",
    "\n",
    "    cutpoint0_stats = np.round(np.quantile(cutpoint0, [0.025, 0.5, 0.975]), 2)\n",
    "    cutpoint1_stats = np.round(np.quantile(cutpoint1, [0.025, 0.5, 0.975]), 2)\n",
    "\n",
    "    print(\"Cutpoint 0:\", cutpoint0_stats[1], f\"(95\\\\% CI = [{cutpoint0_stats[0]}, {cutpoint0_stats[2]}])\")\n",
    "    print(\"Cutpoint 1:\", cutpoint1_stats[1], f\"(95\\\\% CI = [{cutpoint1_stats[0]}, {cutpoint1_stats[2]}])\")\n",
    "\n",
    "    return cutpoint0_stats, cutpoint1_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = cutpoint_stats(results_nat.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between manual generation and automatic generation\n",
    "\n",
    "In Section 4.5 of the paper, we discuss the differences between the ratings for manually generated items that were constructed to be natural and those for automatically generated items. The difference between the manually generated natural, typical items can be read directly off the coefficient estimates. The difference between the manually generated natural, typical items and the automatically generated items needs to be calculated from the posterior samples.\n",
    "\n",
    "To assess whether the automatic generation methods produce examples that are more natural than the manually generated natural, atypical examples, we compute the posterior distribution of the difference in the automatic effects and the manual natural-atypical effect, then test whether it is greater than 0. The proportion of samples on which it is greate than 0 gives us the posterior $p$ that the automatic effects are indeed more positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_atypical_effect = results_nat.fit.stan_variable(\"fixed_coefs\")[:,[1]]\n",
    "automatic_effects = results_nat.fit.stan_variable(\"fixed_coefs\")[:,4:]\n",
    "\n",
    "mean_difference_gt0 = ((automatic_effects - natural_atypical_effect) > 0).mean(0)\n",
    "\n",
    "for coef_name, d in zip(generation_levels[4:], mean_difference_gt0):\n",
    "    if d > 0.99:\n",
    "        print(coef_map[coef_name], \"(posterior $p >$ 0.99)\")\n",
    "    else:\n",
    "        print(coef_map[coef_name], f\"(posterior $p = $ {np.round(d, 2)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting naturalness from frequency\n",
    "\n",
    "In Section 4.6 of the paper, we report regressions assessing whether our automated methods are frequency-sensitiv with respect to naturalness, finding that they are not. These regressions are conducted below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nat_freqsub = data_nat.query('generation_method_type != \"manual\" or generation_method == \"manual_natural_typical\"')\n",
    "\n",
    "data_nat_freqsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_subtype_levels = [\n",
    "    \"manual\", \"reddit\", \"llama\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if glob(\"fits/nat/frequency/*.csv\"):\n",
    "    results_nat_freq = load_fit(\n",
    "        \"fits/nat/frequency/*.csv\",\n",
    "        data_nat_freqsub,\n",
    "        fixed_formula=\"~ 1 + C(generation_method_subtype, levels=generation_subtype_levels) * freq_z\",\n",
    "        by_subj_formula=\"~ 1 + freq_z\", # cannot fit anything bigger, because not all subjects saw items from every generation method\n",
    "        by_verb_formula=\"~ 1\", # cannot fit anything bigger, because not all verbs show up with each generation method\n",
    "        by_sense_formula=\"~ 1\", # cannot fit anything bigger, because not all verb senses show up with each generation method\n",
    "        by_item_formula=\"~ 1\", # cannot fit anything bigger, because items are specific to generation method\n",
    "        item_cols = [\"sentence\"],\n",
    "        subj_cols = [\"rater_id\"],\n",
    "        verb_cols = [\"verb\"],\n",
    "        sense_cols = [\"verb\", \"sense\"]\n",
    "    )\n",
    "else:\n",
    "    results_nat_freq = fit_hmc(\n",
    "        data_nat_freqsub,\n",
    "        fixed_formula=\"~ 1 + C(generation_method_subtype, levels=generation_subtype_levels) * freq_z\",\n",
    "        by_subj_formula=\"~ 1 + freq_z\", # cannot fit anything bigger, because not all subjects saw items from every generation method\n",
    "        by_verb_formula=\"~ 1\", # cannot fit anything bigger, because not all verbs show up with each generation method\n",
    "        by_sense_formula=\"~ 1\", # cannot fit anything bigger, because not all verb senses show up with each generation method\n",
    "        by_item_formula=\"~ 1\", # cannot fit anything bigger, because items are specific to generation method\n",
    "        item_cols = [\"sentence\"],\n",
    "        subj_cols = [\"rater_id\"],\n",
    "        verb_cols = [\"verb\"],\n",
    "        sense_cols = [\"verb\", \"sense\"],\n",
    "        output_dir=\"fits/nat/frequency\",\n",
    "        **sampler_params\n",
    "    )\n",
    "\n",
    "    results_nat_freq.fit.diagnose()\n",
    "\n",
    "    # handles a bug in cmdstanpy.from_csv that I suspect has to do with a version change in STAN\n",
    "    # basically, I *think* STAN used to dump the save_warmup flag as an int, but now it uses a boolean\n",
    "    !find . -type f -wholename './fits/nat/frequency/*.csv' | xargs sed -i 's/save_warmup = false/save_warmup = 0/g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nat_freq.fixed_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_map[\"freq_z\"] = \"Frequency\"\n",
    "\n",
    "def process_coef_name_freq(coef: str) -> str:\n",
    "    if coef == \"Intercept\":\n",
    "        return coef\n",
    "    else:\n",
    "        return \" $\\\\times$ \".join(\n",
    "            coef_map[v] for vs in re.findall(\n",
    "                \"(?:C\\(generation_method_subtype, levels=generation_subtype_levels\\)\\[T\\.(.*?)\\])?:?(.*)\", \n",
    "                coef\n",
    "            ) for v in vs if v\n",
    "        )\n",
    "\n",
    "print_fixed_coef_tabular(results_nat_freq, process_coef_name_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cutpoint_stats(results_nat_freq.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Typicality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin analysis of the typicality experiment by loading and combining the typicality datasets into a single `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ_fnames = [\n",
    "    \"typicality_llama-propbanksenses.csv\",\n",
    "    \"typicality_llama-llamasenses.csv\",\n",
    "    \"typicality_reddit.csv\",\n",
    "    \"typicality_manual.csv\"\n",
    "]\n",
    "\n",
    "data_typ = []\n",
    "\n",
    "for fname in typ_fnames:\n",
    "    df = pd.read_csv(f\"data/{fname}\")\n",
    "\n",
    "    generation_method = fname.replace(\"typicality_\", \"\").strip(\".csv\")\n",
    "\n",
    "    if generation_method == \"manual\":\n",
    "        df[\"list_type\"] = \"manual\"\n",
    "        df[\"generation_method_type\"] = \"manual\"\n",
    "        df[\"generation_method_subtype\"] = \"manual\"\n",
    "        df[\"generation_method_subsubtype\"] = \"manual\"\n",
    "        df = df.query(\"sentence_type == 'target'\")\n",
    "        df[\"generation_method\"] = \"manual_\" + df.naturalness + \"_\" + df.typicality\n",
    "    else:\n",
    "        df[\"list_type\"] = generation_method\n",
    "        df[\"generation_method_type\"] = df.sentence_type.map(lambda x: \"manual\" if x == \"calibration\" else \"automated\")\n",
    "        df[\"generation_method_subtype\"] = df.sentence_type.map(\n",
    "            lambda x: \"manual\" if x == \"calibration\" else generation_method.split(\"-\")[0]\n",
    "        )\n",
    "        df[\"generation_method_subsubtype\"] = df.sentence_type.map(\n",
    "            lambda x: \"manual\" if x == \"calibration\" else generation_method\n",
    "        )\n",
    "        df[\"generation_method\"] = df[[\"sentence_type\", \"naturalness\", \"typicality\"]].agg(\n",
    "            lambda x: \"manual_\" + x.naturalness + \"_\" + x.typicality if x.sentence_type == \"calibration\" else generation_method,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    data_typ.append(df)\n",
    "\n",
    "data_typ = pd.concat(data_typ)\n",
    "\n",
    "data_typ[\"rating_z\"] = data_typ.groupby(\"rater_id\").rating.transform(\n",
    "    lambda x: (x - x.mean())/x.std()\n",
    ")\n",
    "\n",
    "data_typ[\"surprisal_z\"] = (data_typ.surprisal - data_typ.surprisal.mean())/data_typ.surprisal.std()\n",
    "data_typ[\"freq_z\"] = (data_typ.freq - data_typ.freq.mean())/data_typ.freq.std()\n",
    "\n",
    "data_typ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant filtration\n",
    "\n",
    "Some participants did not complete the survey. We remove these participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_counts_typ = {\n",
    "    \"llama-llamasense\": 97,\n",
    "    \"llama-propbanksense\": 97,\n",
    "    \"manual\": 124,\n",
    "    \"reddit\": 72\n",
    "}\n",
    "\n",
    "data_typ = filter_participants(data_typ, expected_counts_typ)\n",
    "\n",
    "data_typ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting\n",
    "\n",
    "We conduct the typicality model fits using the same setup as we used for naturalness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if glob(\"fits/typ/base/*.csv\"):\n",
    "    results_typ = load_fit(\n",
    "        \"fits/typ/base/*.csv\", \n",
    "        data_nat,\n",
    "        fixed_formula=\"~ 1 + C(generation_method, levels=generation_levels)\",\n",
    "        by_subj_formula=\"~ 1\", # cannot fit anything bigger, because not all subjects saw items from every generation method\n",
    "        by_verb_formula=\"~ 1\", # cannot fit anything bigger, because not all verbs show up with each generation method\n",
    "        by_sense_formula=\"~ 1\", # cannot fit anything bigger, because not all verb senses show up with each generation method\n",
    "        by_item_formula=\"~ 1\", # cannot fit anything bigger, because items are specific to generation method\n",
    "        item_cols = [\"sentence\"],\n",
    "        subj_cols = [\"rater_id\"],\n",
    "        verb_cols = [\"verb\"],\n",
    "        sense_cols = [\"verb\", \"sense\"],\n",
    "    )\n",
    "else:\n",
    "    results_typ = fit_hmc(\n",
    "        data_typ,\n",
    "        fixed_formula=\"~ 1 + C(generation_method, levels=generation_levels)\",\n",
    "        by_subj_formula=\"~ 1\", # cannot fit anything bigger, because not all subjects saw items from every generation method\n",
    "        by_verb_formula=\"~ 1\", # cannot fit anything bigger, because not all verbs show up with each generation method\n",
    "        by_sense_formula=\"~ 1\", # cannot fit anything bigger, because not all verb senses show up with each generation method\n",
    "        by_item_formula=\"~ 1\", # cannot fit anything bigger, because items are specific to generation method\n",
    "        item_cols = [\"sentence\"],\n",
    "        subj_cols = [\"rater_id\"],\n",
    "        verb_cols = [\"verb\"],\n",
    "        sense_cols = [\"verb\", \"sense\"],\n",
    "        output_dir=\"fits/typ/base/\",\n",
    "        **sampler_params\n",
    "    )\n",
    "\n",
    "    results_typ.fit.diagnose()\n",
    "\n",
    "    # handles a bug in cmdstanpy.from_csv that I suspect has to do with a version change in STAN\n",
    "    # basically, I *think* STAN used to dump the save_warmup flag as an int, but now it uses a boolean\n",
    "    !find . -type f -wholename './fits/typ/base/*.csv' | xargs sed -i 's/save_warmup = false/save_warmup = 0/g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_typ.fixed_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_fixed_coef_tabular(results_typ, process_coef_name_nat_typ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cutpoint_stats(results_typ.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting typicality from frequency\n",
    "\n",
    "In Section 5.6 of the paper, we report regressions assessing whether our automated methods are frequency-sensitive with respect to typicality, finding that they are not. These regressions are conducted below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_typ_freqsub = data_typ.query('generation_method_type != \"manual\" or generation_method == \"manual_natural_typical\"')\n",
    "\n",
    "data_typ_freqsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if glob(\"fits/typ/frequency/*.csv\"):\n",
    "    results_typ_freq = load_fit(\n",
    "        \"fits/typ/frequency/*.csv\",\n",
    "        data_typ_freqsub,\n",
    "        fixed_formula=\"~ 1 + C(generation_method_subtype, levels=generation_subtype_levels) * freq_z\",\n",
    "        by_subj_formula=\"~ 1 + freq_z\", # cannot fit anything bigger, because not all subjects saw items from every generation method\n",
    "        by_verb_formula=\"~ 1\", # cannot fit anything bigger, because not all verbs show up with each generation method\n",
    "        by_sense_formula=\"~ 1\", # cannot fit anything bigger, because not all verb senses show up with each generation method\n",
    "        by_item_formula=\"~ 1\", # cannot fit anything bigger, because items are specific to generation method\n",
    "        item_cols = [\"sentence\"],\n",
    "        subj_cols = [\"rater_id\"],\n",
    "        verb_cols = [\"verb\"],\n",
    "        sense_cols = [\"verb\", \"sense\"]\n",
    "    )\n",
    "else:\n",
    "    results_typ_freq = fit_hmc(\n",
    "        data_typ_freqsub,\n",
    "        fixed_formula=\"~ 1 + C(generation_method_subtype, levels=generation_subtype_levels) * freq_z\",\n",
    "        by_subj_formula=\"~ 1 + freq_z\", # cannot fit anything bigger, because not all subjects saw items from every generation method\n",
    "        by_verb_formula=\"~ 1\", # cannot fit anything bigger, because not all verbs show up with each generation method\n",
    "        by_sense_formula=\"~ 1\", # cannot fit anything bigger, because not all verb senses show up with each generation method\n",
    "        by_item_formula=\"~ 1\", # cannot fit anything bigger, because items are specific to generation method\n",
    "        item_cols = [\"sentence\"],\n",
    "        subj_cols = [\"rater_id\"],\n",
    "        verb_cols = [\"verb\"],\n",
    "        sense_cols = [\"verb\", \"sense\"],\n",
    "        output_dir=\"fits/typ/frequency\",\n",
    "        **sampler_params\n",
    "    )\n",
    "\n",
    "    results_typ_freq.fit.diagnose()\n",
    "\n",
    "    # handles a bug in cmdstanpy.from_csv that I suspect has to do with a version change in STAN\n",
    "    # basically, I *think* STAN used to dump the save_warmup flag as an int, but now it uses a boolean\n",
    "    !find . -type f -wholename './fits/typ/frequency/*.csv' | xargs sed -i 's/save_warmup = false/save_warmup = 0/g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_typ_freq.fixed_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_fixed_coef_tabular(results_typ_freq, process_coef_name_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cutpoint_stats(results_typ_freq.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict naturalness from typicality\n",
    "\n",
    "In Section 5.6 of the paper, we investigate the hypothesis that typicality plays a role in the degraded naturalness of the automatically generated sentences, finding that it does. These regressions are conducted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_typ_mean = data_typ.pivot_table(\n",
    "    index=\"sentence\", values=\"rating_z\", aggfunc=np.mean\n",
    ").rename(\n",
    "    columns={\"rating_z\": \"typicality_rating\"}\n",
    ").reset_index()\n",
    "\n",
    "data_typ_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nat_typ = pd.merge(data_nat, data_typ_mean, how=\"left\")\n",
    "\n",
    "data_nat_typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if glob(\"fits/nat/typicality/*.csv\"):\n",
    "    results_nat_typ = load_fit(\n",
    "        \"fits/nat/typicality/*.csv\", \n",
    "        data_nat_typ,\n",
    "        fixed_formula=\"~ 1 + typicality_rating\",\n",
    "        by_subj_formula=\"~ 1 + typicality_rating\",\n",
    "        by_verb_formula=\"~ 1 + typicality_rating\",\n",
    "        by_sense_formula=\"~ 1 + typicality_rating\", \n",
    "        by_item_formula=\"~ 1\", # cannot fit anything bigger, because each sentence has only one typicality rating\n",
    "        item_cols = [\"sentence\"],\n",
    "        subj_cols = [\"rater_id\"],\n",
    "        verb_cols = [\"verb\"],\n",
    "        sense_cols = [\"verb\", \"sense\"],\n",
    "    )\n",
    "else:\n",
    "    results_nat_typ = fit_hmc(\n",
    "        data_nat_typ,\n",
    "        fixed_formula=\"~ 1 + typicality_rating\",\n",
    "        by_subj_formula=\"~ 1 + typicality_rating\",\n",
    "        by_verb_formula=\"~ 1 + typicality_rating\",\n",
    "        by_sense_formula=\"~ 1 + typicality_rating\", \n",
    "        by_item_formula=\"~ 1\", # cannot fit anything bigger, because each sentence has only one typicality rating\n",
    "        item_cols = [\"sentence\"],\n",
    "        subj_cols = [\"rater_id\"],\n",
    "        verb_cols = [\"verb\"],\n",
    "        sense_cols = [\"verb\", \"sense\"],\n",
    "        output_dir=\"fits/nat/typicality\",\n",
    "        **sampler_params\n",
    "    )\n",
    "\n",
    "    results_nat_typ.fit.diagnose()\n",
    "\n",
    "    # handles a bug in cmdstanpy.from_csv that I suspect has to do with a version change in STAN\n",
    "    # basically, I *think* STAN used to dump the save_warmup flag as an int, but now it uses a boolean\n",
    "    !find . -type f -wholename './fits/nat/typicality/*.csv' | xargs sed -i 's/save_warmup = false/save_warmup = 0/g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nat_typ.fixed_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_mean = np.round(results_nat_typ.fixed_coefs.post_mean[1], 2)\n",
    "cilo = np.round(results_nat_typ.fixed_coefs[\"2.5%\"][1], 2)\n",
    "cihi = np.round(results_nat_typ.fixed_coefs[\"97.5%\"][1], 2)\n",
    "\n",
    "print(f\"($\\\\beta=${post_mean}, 95\\\\% CrI=[{cilo}, {cihi}])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Distinctiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_fnames = [\n",
    "    \"difference_llama-propbanksenses.csv\",\n",
    "    \"difference_llama-llamasenses.csv\",\n",
    "    \"difference_reddit.csv\",\n",
    "    \"difference_manual.csv\"\n",
    "]\n",
    "\n",
    "data_diff = []\n",
    "\n",
    "for fname in diff_fnames:\n",
    "    df = pd.read_csv(f\"data/{fname}\")\n",
    "\n",
    "    generation_method = fname.replace(\"difference_\", \"\").strip(\".csv\")\n",
    "\n",
    "    if generation_method == \"manual\":\n",
    "        df[\"list_type\"] = \"manual\"\n",
    "        df[\"generation_method_type\"] = \"manual\"\n",
    "        df[\"generation_method_subtype\"] = \"manual\"\n",
    "        df[\"generation_method_subsubtype\"] = \"manual\"\n",
    "        df = df.query(\"pair_type == 'target'\")\n",
    "        df[\"generation_method\"] = \"manual\"\n",
    "    else:\n",
    "        df[\"list_type\"] = generation_method\n",
    "        df[\"generation_method_type\"] = df.pair_type.map(lambda x: \"manual\" if x == \"calibration\" else \"automated\")\n",
    "        df[\"generation_method_subtype\"] = df.pair_type.map(\n",
    "            lambda x: \"manual\" if x == \"calibration\" else generation_method.split(\"-\")[0]\n",
    "        )\n",
    "        df[\"generation_method_subsubtype\"] = df.pair_type.map(\n",
    "            lambda x: \"manual\" if x == \"calibration\" else generation_method\n",
    "        )\n",
    "        df[\"generation_method\"] = df[\"generation_method_subsubtype\"]\n",
    "\n",
    "    data_diff.append(df)\n",
    "\n",
    "data_diff = pd.concat(data_diff)\n",
    "\n",
    "data_diff[\"surprisal1_z\"] = (data_diff.surprisal1 - data_diff.surprisal1.mean())/data_diff.surprisal1.std()\n",
    "data_diff[\"surprisal2_z\"] = (data_diff.surprisal2 - data_diff.surprisal2.mean())/data_diff.surprisal2.std()\n",
    "\n",
    "data_diff[\"freq_z\"] = (data_diff.freq - data_diff.freq.mean())/data_diff.freq.std()\n",
    "\n",
    "data_diff[\"logfreq\"] = np.log1p(data_diff[\"freq\"].fillna(0))\n",
    "data_diff[\"logfreq_z\"] = (data_diff.logfreq - data_diff.logfreq.mean())/data_diff.logfreq.std()\n",
    "\n",
    "if data_diff.sense1.isnull().any() or data_diff.sense2.isnull().any():\n",
    "    print(\"Inserting random dummy senses.\")\n",
    "    data_diff[\"sense1\"] = np.random.choice(100, size=data_diff.shape[0]).astype(str)\n",
    "    data_diff[\"sense2\"] = np.random.choice(100, size=data_diff.shape[0]).astype(str)\n",
    "\n",
    "if (data_diff.comparison == \"filler\").any():\n",
    "    data_diff_automated_calibration = data_diff[data_diff.pair_type==\"filler\"].drop(columns=[\"comparison\"])\n",
    "\n",
    "    data_diff_automated_calibration = pd.merge(\n",
    "        data_diff_automated_calibration ,\n",
    "        data_diff[data_diff.generation_method==\"manual\"][[\"sentence1\", \"sentence2\", \"comparison\"]].drop_duplicates()\n",
    "    )\n",
    "\n",
    "    data_diff = pd.concat(\n",
    "        [data_diff[data_diff.pair_type != \"filler\"], data_diff_automated_calibration],\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "data_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_counts_diff = {\n",
    "    \"llama-llamasense\": 68,\n",
    "    \"llama-propbanksense\": 68,\n",
    "    \"manual\": 62,\n",
    "    \"reddit\": 68\n",
    "}\n",
    "\n",
    "data_diff = filter_participants(data_diff, expected_counts_diff)\n",
    "\n",
    "data_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_levels = [\n",
    "    'manual', 'reddit', \n",
    "    'llama-propbanksense', 'llama-llamasense'\n",
    "]\n",
    "\n",
    "comparison_levels = ['same', 'different']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if glob(\"fits/pairdiff/base/*.csv\"):\n",
    "    results_diff = load_fit(\n",
    "        \"fits/pairdiff/base/*.csv\", \n",
    "        data_diff,\n",
    "        fixed_formula=\"~ 1 + C(comparison, levels=comparison_levels) * C(generation_method, levels=generation_levels)\",\n",
    "        by_subj_formula=\"~ 1 + C(comparison, levels=comparison_levels)\",\n",
    "        by_verb_formula=\"~ 1 + C(comparison, levels=comparison_levels)\",\n",
    "        by_sense_formula=\"~ 1\", \n",
    "        by_item_formula=\"~ 1\",\n",
    "        item_cols=[\"sentence1\", \"sentence2\"],\n",
    "        sense_cols = [\"verb\", \"sense1\", \"sense2\"],\n",
    "        subj_cols=[\"rater_id\"],\n",
    "        verb_cols = [\"verb\"],\n",
    "    )\n",
    "else:\n",
    "    results_diff = fit_hmc(\n",
    "        data_diff,\n",
    "        fixed_formula=\"~ 1 + C(comparison, levels=comparison_levels) * C(generation_method, levels=generation_levels)\",\n",
    "        by_subj_formula=\"~ 1 + C(comparison, levels=comparison_levels)\",\n",
    "        by_verb_formula=\"~ 1 + C(comparison, levels=comparison_levels)\",\n",
    "        by_sense_formula=\"~ 1\", \n",
    "        by_item_formula=\"~ 1\",\n",
    "        item_cols=[\"sentence1\", \"sentence2\"],\n",
    "        sense_cols = [\"verb\", \"sense1\", \"sense2\"],\n",
    "        subj_cols=[\"rater_id\"],\n",
    "        verb_cols = [\"verb\"],\n",
    "        output_dir=\"fits/pairdiff/base/\",\n",
    "        **sampler_params\n",
    "    )\n",
    "\n",
    "    results_diff.fit.diagnose()\n",
    "\n",
    "    # handles a bug in cmdstanpy.from_csv that I suspect has to do with a version change in STAN\n",
    "    # basically, I *think* STAN used to dump the save_warmup flag as an int, but now it uses a boolean\n",
    "    !find . -type f -wholename './fits/pairdiff/base/*.csv' | xargs sed -i 's/save_warmup = false/save_warmup = 0/g'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diff.fixed_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_coef_name_pairdiff(coef: str) -> str:\n",
    "    coef_map = {\n",
    "        \"manual\": \"Manual\",\n",
    "        \"reddit\": \"Corpus\",\n",
    "        \"llama-propbanksense\": \"LM with PropBank senses\",\n",
    "        \"llama-llamasense\": \"LM with LM senses\",\n",
    "        \"different\": \"Different Sense\",\n",
    "        \"same\": \"Same Sense\"\n",
    "    }\n",
    "\n",
    "    if coef == \"Intercept\":\n",
    "        return coef\n",
    "    else:\n",
    "        return \" $\\\\times$ \".join(\n",
    "            coef_map[v] for v in re.findall(\n",
    "                \"C\\(.*?\\)\\[T\\.(.*?)\\]\", \n",
    "                coef\n",
    "            )\n",
    "        )\n",
    "\n",
    "print_fixed_coef_tabular(results_diff, process_coef_name_pairdiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = results_diff.fit.stan_variable(\"fixed_coefs\")[:,0]\n",
    "different_effect = results_diff.fit.stan_variable(\"fixed_coefs\")[:,1]\n",
    "corpus_effect = results_diff.fit.stan_variable(\"fixed_coefs\")[:,2]\n",
    "corpus_different_effect = results_diff.fit.stan_variable(\"fixed_coefs\")[:,5]\n",
    "\n",
    "manual_different = intercept + different_effect\n",
    "corpus_different = intercept + different_effect + corpus_effect + corpus_different_effect\n",
    "\n",
    "((corpus_different - manual_different) < 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbsense_effect = results_diff.fit.stan_variable(\"fixed_coefs\")[:,3]\n",
    "lmsense_effect = results_diff.fit.stan_variable(\"fixed_coefs\")[:,4]\n",
    "pbsense_different_effect = results_diff.fit.stan_variable(\"fixed_coefs\")[:,6]\n",
    "lmsense_different_effect = results_diff.fit.stan_variable(\"fixed_coefs\")[:,7]\n",
    "\n",
    "pbsense_different = intercept + different_effect + pbsense_effect + pbsense_different_effect\n",
    "lmsense_different = intercept + different_effect + lmsense_effect + lmsense_different_effect\n",
    "\n",
    "((lmsense_different - pbsense_different) < 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((pbsense_different - manual_different) < 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((corpus_different - pbsense_different) < 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cutpoint_stats(results_diff.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutpoint1_pairdiff = cutpoint0_pairdiff + np.exp(fit_pairdiff[\"fit\"].stan_variable(\"interval_size_logmean\"))\n",
    "\n",
    "print(\n",
    "    np.round(cutpoint1_pairdiff.mean(0), 2), \n",
    "    f\"[{np.round(np.quantile(cutpoint1_pairdiff, 0.025), 2)}, {np.round(np.quantile(cutpoint1_pairdiff, 0.975), 2)}]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = arviz.plot_posterior(fit_pairdiff[\"fit\"], var_names=[\"fixed_coefs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = arviz.plot_posterior(fit_pairdiff[\"fit\"], var_names=[\"subj_scale\", \"item_scale\", \"verb_scale\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = arviz.plot_posterior(fit_pairdiff[\"fit\"], var_names=[\"sample_size\", \"interval_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_pairdiff_surprisal = fit_hmc(\n",
    "    data_diff,\n",
    "    fixed_formula=(\"~ 1 + C(PairType, levels=comparison_levels) * surprisal1_z * surprisal2_z\"\n",
    "                      \" + C(PairType, levels=comparison_levels) * typicality1 * typicality2\" \n",
    "                      \" + C(PairType, levels=comparison_levels) * freq\"),\n",
    "    by_subj_formula=\"~ 1 + C(PairType, levels=comparison_levels)\",\n",
    "    by_verb_formula=\"~ 1 + C(PairType, levels=comparison_levels)\",\n",
    "    by_sense_formula=\"~ 1\", \n",
    "    by_item_formula=\"~ 1\", \n",
    "    max_init_iter=0,\n",
    "    item_cols=[\"sentence1\", \"sentence2\"],\n",
    "    sense_cols = [\"verb\", \"sense1\", \"sense2\"],\n",
    "    subj_cols=[\"rater_id\"],\n",
    "    verb_cols = [\"verb\"],\n",
    "    output_dir=\"fits/pairdiff/\",\n",
    "    **sampler_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_pairdiff_surprisal[\"fixed_coefs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_fixed_coef_tabular(fit_pairdiff_surprisal, process_coef_name_pairdiff_surprisal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = arviz.plot_posterior(fit_pairdiff_surprisal[\"fit\"], var_names=[\"fixed_coefs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = arviz.plot_posterior(fit_pairdiff_surprisal[\"fit\"], var_names=[\"subj_scale\", \"item_scale\", \"verb_scale\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = arviz.plot_posterior(fit_pairdiff_surprisal[\"fit\"], var_names=[\"sample_size\", \"interval_size\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
